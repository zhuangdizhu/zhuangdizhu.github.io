<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Zhuangdi Zhu</title>
    <link>https://zhuangdizhu.github.io/tag/machine-learning/</link>
      <atom:link href="https://zhuangdizhu.github.io/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2023 Zhuangdi Zhu</copyright><lastBuildDate>Mon, 27 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zhuangdizhu.github.io/media/icon_hucab448affb9213f36d8e962fe68c6b19_391842_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning</title>
      <link>https://zhuangdizhu.github.io/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Federated Learning</title>
      <link>https://zhuangdizhu.github.io/project/federated-learning/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://zhuangdizhu.github.io/project/federated-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluate Binary Classification with Keras</title>
      <link>https://zhuangdizhu.github.io/post/keras-eval-binary-classification/</link>
      <pubDate>Thu, 15 Nov 2018 23:24:41 -0500</pubDate>
      <guid>https://zhuangdizhu.github.io/post/keras-eval-binary-classification/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;keras.io&#34;&gt;Keras&lt;/a&gt; provides very convenient tools for fast protyping Machine Learning models, especially neural networks. You can pass &lt;a href=&#34;https://keras.io/metrics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;metric functions&lt;/a&gt; when compiling a model, to evaluate the learnt models. However in the current version (after v2.0.0), Keras no longer provides widely used binary-classification metrics, e.g., recall, f1score, etc. The reason is clearly explained in &lt;a href=&#34;https://github.com/keras-team/keras/issues/5794#issuecomment-287641301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keras issue #5794&lt;/a&gt;. In this posts, we are going to dicuss a working-around to evaluate these metrics with Keras.&lt;/p&gt;
&lt;!--more---&gt;
&lt;h2 id=&#34;why-not-use-global-metrics&#34;&gt;Why not use global metrics&lt;/h2&gt;
&lt;p&gt;That is the metrics evaluated in Keras are batch-wise only. The epoch output metric values are averaged like: $${\sum \text{(batch metric)} * \text{(batch size)} \over \text{(# batch)} * \text{(batch size)}}$$. This is okay for batch-wise metric like accuracy: $${\sum \text{(batch #TP + #TN)/(batch size)} * \text{(batch size)} \over \text{(# batch)} * \text{(batch size)}} = {\text{(#TP + #TN)} \over \text{(total #sample)}}$$. For global metrics, e.g., recall, the average is improper: $${\sum \text{(batch #TP)/(batch #TP + #FN)} * \text{(batch size)} \over \text{(# batch)} * \text{(batch size)}} \neq {\text{(#TP)} \over \text{(total #TP + #FN)}}$$. Of course, when you use a relatively large batch size and a large number of batches, the average will be close to the global value. However, there will aften be a large gap between the average and global value. Especially for AUC value, the computation is danguous.&lt;/p&gt;
&lt;p&gt;If you don&amp;rsquo;t mind the bias caused by the average, you could use metrics passed to &lt;code&gt;model.compile(...)&lt;/code&gt;, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def recall(y_true, y_pred, is_categorical=True):
    &amp;quot;&amp;quot;&amp;quot;Recall metric.	
     Only computes a batch-wise average of recall.	
     Computes the recall, a metric for multi-label classification of	
    how many relevant items are selected.	
    &amp;quot;&amp;quot;&amp;quot;
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is notable that the &lt;code&gt;K.epsilon()&lt;/code&gt; has to be used in the division, because &lt;code&gt;possible_positives&lt;/code&gt; could be zero in one batch.&lt;/p&gt;
&lt;h2 id=&#34;existing-solutions&#34;&gt;Existing solutions&lt;/h2&gt;
&lt;h3 id=&#34;batch-wise-estimation&#34;&gt;Batch-wise estimation&lt;/h3&gt;
&lt;p&gt;In addtion to the example mentioned above, there are some discussion on the batch-wise estimation.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Decorate tensorflow metrics: &lt;a href=&#34;https://stackoverflow.com/a/50566908/3503604&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to calculate precision and recall in Keras&lt;/a&gt;, which seems not working now.&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;better-practice&#34;&gt;Better Practice&lt;/h2&gt;
&lt;h3 id=&#34;predict-and-evaluate-metrics&#34;&gt;Predict and evaluate metrics&lt;/h3&gt;
&lt;p&gt;Compute the global metric value on epoch end.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;class Metrics(keras.callbacks.Callback):
    def __init__(self, validation_data):
        self.validation_data = validation_data
    def on_epoch_end(self, batch, logs={}):
        predict = np.asarray(self.model.predict(self.validation_data[0]))
        targ = self.validation_data[1]
        self.f1s=f1(targ, predict)
        return
metrics = Metrics([X_test,y_test])
model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=[X_test,y_test], 
       verbose=1, callbacks=[metrics])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[Source codes from &lt;a href=&#34;https://github.com/keras-team/keras/issues/5794#issuecomment-303683985&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;comment in Keras issue # 5794&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;However, this solution will be time-comsuming to run &lt;code&gt;model.predict&lt;/code&gt; on the epoch end. The prediction procdure is actually evaluated during training. The repitation is wasteful.&lt;/p&gt;
&lt;h3 id=&#34;a-keras-metrics-package&#34;&gt;A keras-metrics package&lt;/h3&gt;
&lt;p&gt;One thought to tackle the issue is to fetch predictions from the model and then evaluate metrics. A close solution is given in &lt;a href=&#34;https://github.com/netrack/keras-metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keras-metrics&lt;/a&gt;. In the package, they create a class to store history record of true positive, false positive and so on. However the test case and example given by the authors cannot demonstrate the effectiveness. One drawback of their solution is that they do not solve the &lt;em&gt;averaging&lt;/em&gt; problem.&lt;/p&gt;
&lt;p&gt;However, the keras-metrics package is only effective in the Keras (&amp;gt;=v2.1.6) which will avoid averaging metrics which are stateful &lt;a href=&#34;https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/keras/engine/base_layer.py#L22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Layer&lt;/a&gt; instances. Look at &lt;a href=&#34;https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/keras/callbacks.py#L204&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BaseLogger&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;official-updates-in-keras-v216-for-stateful-metrics&#34;&gt;Official updates in Keras v2.1.6 for stateful metrics&lt;/h3&gt;
&lt;p&gt;Until today, there has been some updates in Keras. You can find Pull-Request: &lt;a href=&#34;https://github.com/keras-team/keras/pull/9253&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR#9253: Add support for stateful metrics.&lt;/a&gt; and &lt;a href=&#34;https://github.com/keras-team/keras/pull/9446&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR#9446: General stateful metrics fixes&lt;/a&gt;.
However there is still no official metrics for recall, f1score etc.&lt;/p&gt;
&lt;h3 id=&#34;customize-stateful-metrics&#34;&gt;Customize stateful metrics&lt;/h3&gt;
&lt;p&gt;If you have update Keras to &lt;code&gt;v2.1.6&lt;/code&gt; which supports &amp;lsquo;stateful metrics&amp;rsquo;, then you can try to customize some metric like the one in &lt;a href=&#34;https://stackoverflow.com/a/51412555/3503604&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The best reference is the &lt;a href=&#34;https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/tests/keras/metrics_test.py#L127&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BinaryTruePositives&lt;/a&gt; class provided by Keras test case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Differentially Private Learning</title>
      <link>https://zhuangdizhu.github.io/project/private-learning/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://zhuangdizhu.github.io/project/private-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Subspace Learning</title>
      <link>https://zhuangdizhu.github.io/project/subspace-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://zhuangdizhu.github.io/project/subspace-learning/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
